{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from utils import ProgressBar\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 0.01 % [>--------------------------------------------------] 1000/10000000 \t used:0s eta:5 s\r",
      " 0.02 % [>--------------------------------------------------] 2000/10000000 \t used:0s eta:32 s\r",
      " 0.03 % [>--------------------------------------------------] 3000/10000000 \t used:0s eta:38 s\r",
      " 0.04 % [>--------------------------------------------------] 4000/10000000 \t used:0s eta:38 s\r",
      " 0.05 % [>--------------------------------------------------] 5000/10000000 \t used:0s eta:37 s\r",
      " 0.06 % [>--------------------------------------------------] 6000/10000000 \t used:0s eta:37 s\r",
      " 0.07 % [>--------------------------------------------------] 7000/10000000 \t used:0s eta:38 s\r",
      " 0.08 % [>--------------------------------------------------] 8000/10000000 \t used:0s eta:40 s\r",
      " 0.09 % [>--------------------------------------------------] 9000/10000000 \t used:0s eta:43 s\r",
      " 0.10 % [>--------------------------------------------------] 10000/10000000 \t used:0s eta:46 s\r",
      " 0.11 % [>--------------------------------------------------] 11000/10000000 \t used:0s eta:47 s\r",
      " 0.12 % [>--------------------------------------------------] 12000/10000000 \t used:0s eta:48 s\r",
      " 0.13 % [>--------------------------------------------------] 13000/10000000 \t used:0s eta:50 s\r",
      " 0.14 % [>--------------------------------------------------] 14000/10000000 \t used:0s eta:52 s\r",
      " 0.15 % [>--------------------------------------------------] 15000/10000000 \t used:0s eta:52 s\r",
      " 0.16 % [>--------------------------------------------------] 16000/10000000 \t used:0s eta:51 s\r",
      " 0.17 % [>--------------------------------------------------] 17000/10000000 \t used:0s eta:51 s\r",
      " 0.18 % [>--------------------------------------------------] 18000/10000000 \t used:0s eta:50 s\r",
      " 0.19 % [>--------------------------------------------------] 19000/10000000 \t used:0s eta:49 s\r",
      " 0.20 % [>--------------------------------------------------] 20000/10000000 \t used:0s eta:48 s\r",
      " 0.21 % [>--------------------------------------------------] 21000/10000000 \t used:0s eta:48 s\r",
      " 0.22 % [>--------------------------------------------------] 22000/10000000 \t used:0s eta:48 s\r",
      " 0.23 % [>--------------------------------------------------] 23000/10000000 \t used:0s eta:47 s\r",
      " 0.24 % [>--------------------------------------------------] 24000/10000000 \t used:0s eta:47 s\r",
      " 0.25 % [>--------------------------------------------------] 25000/10000000 \t used:0s eta:47 s\r",
      " 0.26 % [>--------------------------------------------------] 26000/10000000 \t used:0s eta:47 s\r",
      " 100.00 % [==================================================>] 10000000/10000000 \t used:0s eta:0 s"
     ]
    }
   ],
   "source": [
    "sentlength = 10\n",
    "limitation = 10000\n",
    "\n",
    "enlines = []\n",
    "START = '<START>'\n",
    "END = '<END>'\n",
    "\n",
    "pb = ProgressBar(worksum=10000000)\n",
    "pb.startjob()\n",
    "num = 0\n",
    "complete = 0\n",
    "# 只取英文句子\n",
    "with open('data/segmented_train_seg_by_word.txt') as fhdl:\n",
    "    for line in fhdl:\n",
    "        num += 1\n",
    "        # 使用行号作为标注，奇数行为英文，偶数行为中文\n",
    "        if num % 2 == 1:\n",
    "            enline = line\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        enlinesp = [START] + [i.lower() for i in enline.strip(\"\\n\").split()] + [END]\n",
    "        # 设置一个最大长度提升demo速度\n",
    "        if len(enlinesp) <= sentlength:\n",
    "            enlines.append(enlinesp)\n",
    "            \n",
    "        if (num // 2) % 1000 == 0:\n",
    "            complete += 1000\n",
    "            pb.complete(1000)\n",
    "        \n",
    "        if len(enlines) >= limitation:\n",
    "            pb.complete(10000000 - complete)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2ind = {}\n",
    "ind2word = {}\n",
    "\n",
    "def addchar(word2ind,ind2word,char):\n",
    "    if char in word2ind:\n",
    "        return \n",
    "    ind2word[len(word2ind)] = char\n",
    "    word2ind[char] = len(word2ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "specialchars = ['<pad>']\n",
    "\n",
    "for word in specialchars:\n",
    "    addchar(word2ind,ind2word, word)\n",
    "\n",
    "for line in enlines:\n",
    "    for word in line:\n",
    "        addchar(word2ind,ind2word,word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_seq_segment(sequences):\n",
    "    seq_segments = list()\n",
    "    for line in sequences:\n",
    "        for i in range(1, len(line)):\n",
    "            seq = line[0:i+1]\n",
    "            seq_segments.append(seq)\n",
    "    return seq_segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_segments = create_seq_segment(enlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "for segment in seq_segments:\n",
    "    seg_ids = [word2ind.get(i) for i in segment]\n",
    "    x.append(seg_ids)\n",
    "    y.append(seg_ids[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = len(word2ind)\n",
    "x_np = tf.keras.preprocessing.sequence.pad_sequences(x,padding='post',value=word2ind['<pad>'])\n",
    "y_np = np.asarray(y)\n",
    "y_np = to_categorical(y_np, num_classes=vocab)\n",
    "x_tr, x_val, y_tr, y_val = train_test_split(x_np, y_np, test_size=0.1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 10, 50)            363950    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 150)               120600    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 7279)              1099129   \n",
      "=================================================================\n",
      "Total params: 1,583,679\n",
      "Trainable params: 1,583,679\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "2257/2257 - 68s - loss: 4.8086 - acc: 0.2746 - val_loss: 3.6073 - val_acc: 0.3825\n",
      "Epoch 2/20\n",
      "2257/2257 - 66s - loss: 3.1531 - acc: 0.4653 - val_loss: 2.5775 - val_acc: 0.5740\n",
      "Epoch 3/20\n",
      "2257/2257 - 65s - loss: 2.4608 - acc: 0.5857 - val_loss: 2.0572 - val_acc: 0.6536\n",
      "Epoch 4/20\n",
      "2257/2257 - 69s - loss: 2.0184 - acc: 0.6557 - val_loss: 1.6546 - val_acc: 0.7154\n",
      "Epoch 5/20\n",
      "2257/2257 - 79s - loss: 1.6809 - acc: 0.7067 - val_loss: 1.3405 - val_acc: 0.7520\n",
      "Epoch 6/20\n",
      "2257/2257 - 72s - loss: 1.4185 - acc: 0.7414 - val_loss: 1.1211 - val_acc: 0.7793\n",
      "Epoch 7/20\n",
      "2257/2257 - 78s - loss: 1.1992 - acc: 0.7716 - val_loss: 0.8886 - val_acc: 0.8262\n",
      "Epoch 8/20\n",
      "2257/2257 - 67s - loss: 1.0088 - acc: 0.7986 - val_loss: 0.7228 - val_acc: 0.8607\n",
      "Epoch 9/20\n",
      "2257/2257 - 66s - loss: 0.8489 - acc: 0.8223 - val_loss: 0.5831 - val_acc: 0.8863\n",
      "Epoch 10/20\n",
      "2257/2257 - 70s - loss: 0.7193 - acc: 0.8436 - val_loss: 0.4725 - val_acc: 0.9094\n",
      "Epoch 11/20\n",
      "2257/2257 - 62s - loss: 0.6058 - acc: 0.8639 - val_loss: 0.3728 - val_acc: 0.9302\n",
      "Epoch 12/20\n",
      "2257/2257 - 61s - loss: 0.5112 - acc: 0.8846 - val_loss: 0.2853 - val_acc: 0.9515\n",
      "Epoch 13/20\n",
      "2257/2257 - 65s - loss: 0.4262 - acc: 0.9024 - val_loss: 0.2287 - val_acc: 0.9680\n",
      "Epoch 14/20\n",
      "2257/2257 - 61s - loss: 0.3535 - acc: 0.9198 - val_loss: 0.1700 - val_acc: 0.9748\n",
      "Epoch 15/20\n",
      "2257/2257 - 61s - loss: 0.2899 - acc: 0.9366 - val_loss: 0.1383 - val_acc: 0.9810\n",
      "Epoch 16/20\n",
      "2257/2257 - 60s - loss: 0.2371 - acc: 0.9499 - val_loss: 0.0976 - val_acc: 0.9874\n",
      "Epoch 17/20\n",
      "2257/2257 - 70s - loss: 0.1896 - acc: 0.9622 - val_loss: 0.0720 - val_acc: 0.9920\n",
      "Epoch 18/20\n",
      "2257/2257 - 64s - loss: 0.1515 - acc: 0.9703 - val_loss: 0.0537 - val_acc: 0.9939\n",
      "Epoch 19/20\n",
      "2257/2257 - 65s - loss: 0.1191 - acc: 0.9775 - val_loss: 0.0371 - val_acc: 0.9960\n",
      "Epoch 20/20\n",
      "2257/2257 - 62s - loss: 0.0998 - acc: 0.9816 - val_loss: 0.0309 - val_acc: 0.9978\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f93160e5f10>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab, 50, input_length=sentlength, trainable=True))\n",
    "model.add(LSTM(150, recurrent_dropout=0.1, dropout=0.1))\n",
    "model.add(Dense(vocab, activation='softmax'))\n",
    "print(model.summary())\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss='categorical_crossentropy', metrics=['acc'], optimizer='adam')\n",
    "# fit the model\n",
    "model.fit(x_np, y_np, epochs=20, verbose=2, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a sequence of characters with a language model\n",
    "def generate_seq(model, ind2word, max_seq_length):\n",
    "    in_text = [word2ind[START]]\n",
    "    new_word = None\n",
    "    while new_word != word2ind[END] and len(in_text) < max_seq_length:\n",
    "        encoded = pad_sequences([in_text], maxlen=max_seq_length, truncating='pre')\n",
    "        word_probs = model.predict(encoded, verbose=0)[0]\n",
    "        r = random.random()\n",
    "        accumulator = .0\n",
    "        \n",
    "        while accumulator < r:\n",
    "            new_word = np.argmax(word_probs)\n",
    "            accumulator += word_probs[new_word]\n",
    "            word_probs[new_word] = 0\n",
    "            \n",
    "        in_text.append(new_word.item())\n",
    "    return ' '.join([ind2word[word] for word in in_text[1:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 hundred keep inconsistencies keep such 15 wanted give dealt\n",
      "1 actually clean easy pen hens large classroom oath friendship\n",
      "2 1000 nobody double large stop kills sit speak garrison\n",
      "3 without keep face freshman nobody bloody rubbish best squadron\n",
      "4 tell 18 15 brilliant vodka 14 65 fault holiday\n",
      "5 fifty keep nobody if missile 15 stop terrible necklace\n",
      "6 18 springs words 18 keep hmm jury wooden guv\n",
      "7 classes hens zip nobody clean weeks kingdom barely papassian\n",
      "8 band saved neither 15 15 called large life continuous\n",
      "9 fifty upon nobody nobody fifty mean nobody kills detect\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    sent = generate_seq(model, ind2word, sentlength)\n",
    "    print(i, sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
